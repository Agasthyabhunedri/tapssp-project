# ğŸ¦€ Rust RAG CLI  
**Retrieval-Augmented Generation System in Rust**  
_Designed by **Agasthya Bhunedri** â€” DePaul University (CSC 595 Â· Systems Programming in Rust)_

---

## ğŸš€ Overview

**Goal**  
Build a **trustworthy, low-latency Q&A system** over your own documents and code using **Rust**.  
The tool **retrieves** relevant text chunks from local files and **grounds** answers produced by an **LLM**, ensuring transparency, speed, and reproducibility with clear source citations.

Instead of retraining or fine-tuning models, this project uses **Retrieval-Augmented Generation (RAG)** to inject private, up-to-date context directly into prompts â€” powered by Rustâ€™s concurrency, safety, and performance guarantees.

---

## ğŸ§  Problem

Large Language Models (LLMs) often **hallucinate** or miss context because they donâ€™t â€œknowâ€ your internal documentation, logs, or code.  
Teams need a **local**, **verifiable**, and **fast** way to query their own files.

This system provides:

- âœ… Local, source-grounded answers  
- âš¡ Low-latency text retrieval and ranking  
- ğŸ§© Modular API traits (Embedder, Retriever, LLM)  
- ğŸ”’ Secure and offline-friendly operation

 ---

## âš™ï¸ Architecture

```text
rag ingest / query / stats
          â”‚
          â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Ingestion Pipeline         â”‚
 â”‚  â†’ Parse .md / .txt / .rs â”‚
 â”‚  â†’ Chunk & store in DB    â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Index Layer                â”‚
 â”‚  â†’ Vector (HNSW) Index     â”‚
 â”‚  â†’ Lexical (BM25) Index    â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Hybrid Retriever + Rerank  â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ LLM Backend (OpenAI)       â”‚
 â”‚  â†’ Context + Citations     â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```
----

## ğŸ§© Example Workflow

### 1ï¸âƒ£ Ingest your documents
```text
rag ingest ./docs ./src --chunk-size 512 --overlap 64
```
**Output**
```text
[âœ”] Loaded 28 files (MD, RS, TXT)
[âœ”] Created 410 chunks (avg 508 chars)
[âœ”] Embedded via text-embedding-3-small (dim 1536)
[âœ”] Indexed to vector and BM25 stores
Workspace : ./data (HNSW 3.1 MB Â· Tantivy 6.7 MB)
```
2ï¸âƒ£ Query the system
```text
rag query "How does log rotation work?" --top-k 6 --cite

```

Output
```text
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Answer:
The log rotation service spawns a background
thread that checks file size and modification
time. Files larger than 10 MB or older than 7 days
are renamed with a timestamp suffix and recreated.
Configuration is in config/log.toml. [1][2]

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Citations:
[1] src/log/rotate.rs (lines 42â€“68)
[2] config/log.toml (lines 1â€“12)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Latency : 1.84 s  Retrieval : 0.72 s  LLM : 1.12 s
```
3ï¸âƒ£ View corpus stats
```text
rag stats
```

Output
```text
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Corpus Stats
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Documents   : 28
Chunks      : 410
Embedding Dim : 1536
Vector Index  : 3.1 MB
Lexical Index : 6.7 MB
p50 Retrieval Latency : 0.72 s
